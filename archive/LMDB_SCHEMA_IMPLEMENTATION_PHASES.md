# LMDB Schema Implementation Phases

**Date:** 2025-11-20
**Status:** Roadmap for Full Schema Population

---

## Vision

A comprehensive legal research platform with:
- **Graph navigation** - Bidirectional citation traversal
- **Visual network** - Corpus-colored nodes, depth-based opacity
- **Semantic enrichment** - AI-generated summaries and analysis
- **LLM research workflows** - Deep contextual analysis

---

## Schema Status by Field

### PRIMARY.LMDB Schema

#### âœ… Phase 1: Core Storage (CURRENTLY POPULATED)

```python
{
  # Basic identifiers
  "section_number": str,        # âœ… Populated
  "url": str,                   # âœ… Populated
  "url_hash": str,              # âœ… Populated

  # Display fields
  "header": str,                # âœ… Populated (full header with title)
  "section_title": str,         # âœ… Populated (extracted from header)

  # Legal text
  "paragraphs": list[str],      # âœ… Populated (preserved original)
  "full_text": str,             # âœ… Populated (joined paragraphs)
  "word_count": int,            # âœ… Populated
  "paragraph_count": int,       # âœ… Populated

  # Citation metadata (for graph)
  "has_citations": bool,        # âœ… Populated (forward citations exist)
  "citation_count": int,        # âœ… Populated (number of forward refs)
  "in_complex_chain": bool,     # âœ… Populated (in chains.lmdb)
  "is_clickable": bool,         # âœ… Populated (has forward OR reverse citations)

  # Timestamp
  "scraped_date": str,          # âœ… Populated (ISO8601)
}
```

**Status:** COMPLETE âœ…
**Location:** All 4 corpora (`build_comprehensive_lmdb.py`)

---

#### ðŸ”¨ Phase 2: Extractable Metadata (NEXT PRIORITY)

```python
{
  # Organizational hierarchy (extractable from existing data)
  "chapter": str | None,        # ðŸ“‹ TODO: Extract from section_number or header
  "title": str | None,          # ðŸ“‹ TODO: Extract from header structure

  # Temporal validity (basic implementation)
  "valid_from": str | None,     # ðŸ“‹ TODO: Extract from scraped_date or metadata
  "valid_until": str | None,    # ðŸ“‹ TODO: Set to null (assume current = valid)
  "treatment_status": str | None, # ðŸ“‹ TODO: Default to "valid" for statutes

  # Case law specific (corpus-dependent)
  "court_level": str | None,    # ðŸ“‹ TODO: Extract from case metadata (ohio_caselaw only)
  "binding_on": list[str] | None, # ðŸ“‹ TODO: Compute from court hierarchy
  "precedent_value": str | None, # ðŸ“‹ TODO: Derive from court_level
}
```

**Implementation Notes:**

1. **Chapter/Title Extraction:**
   - Ohio Revised Code: Section "2913.02" â†’ Chapter "29" (Criminal Code)
   - Admin Code: Rule "3701-17-01" â†’ Chapter "3701-17"
   - Constitution: "Article I, Section 1" â†’ Article "I"
   - Case Law: Extract from case metadata

2. **Treatment Status Logic:**
   ```python
   # Statutes: default to "valid" (assume current law is valid)
   treatment_status = "valid"

   # Case law: could be "valid", "overruled", "questioned", "superseded"
   # Start with "valid", update when relationship tracking is built
   ```

3. **Court Hierarchy (Case Law Only):**
   ```python
   court_levels = {
       "supreme_court": {
           "binding_on": ["all_ohio_courts"],
           "precedent_value": "binding"
       },
       "appellate": {
           "binding_on": ["trial_courts_in_district"],
           "precedent_value": "persuasive"  # for other districts
       },
       "trial": {
           "binding_on": [],
           "precedent_value": "persuasive"
       }
   }
   ```

**Effort:** MEDIUM (extraction logic + corpus-specific handling)
**Benefit:** Organizational context, basic temporal tracking

---

#### ðŸ§® Phase 3: Graph Metrics (COMPUTE FROM NETWORK)

```python
{
  # Network analysis (computed from citation graph)
  "authority_score": float | None,      # ðŸ”® TODO: PageRank or HITS authority
  "betweenness_centrality": float | None, # ðŸ”® TODO: NetworkX centrality
  "citation_velocity": float | None,    # ðŸ”® TODO: Citations per year (recent)
}
```

**Implementation Approach:**

1. **Build Citation Graph:**
   ```python
   import networkx as nx

   # Create directed graph from citations.lmdb and reverse_citations.lmdb
   G = nx.DiGraph()

   # Add all sections as nodes
   for section in primary_db:
       G.add_node(section_id)

   # Add edges from citation relationships
   for section, citations in citations_db:
       for cited in citations:
           G.add_edge(section, cited)
   ```

2. **Compute Authority (PageRank):**
   ```python
   # Higher score = more cited by important sections
   authority_scores = nx.pagerank(G, alpha=0.85)
   ```

3. **Compute Centrality:**
   ```python
   # Higher score = more important for connecting parts of the graph
   centrality = nx.betweenness_centrality(G)
   ```

4. **Citation Velocity:**
   ```python
   # Count citations per year (requires case dates)
   def compute_velocity(section_id, lookback_years=5):
       citing_cases = get_citing_cases_with_dates(section_id)
       recent = [c for c in citing_cases if c.year >= current_year - lookback_years]
       return len(recent) / lookback_years
   ```

**Effort:** HIGH (requires networkx, graph computation pipeline)
**Benefit:** Authority ranking, importance scoring for search results

---

#### ðŸ¤– Phase 4: AI Enrichment (LLM-GENERATED)

```python
{
  "enrichment": {
    # Semantic understanding (AI-generated)
    "summary": str | None,              # ðŸ¤– TODO: LLM 2-sentence plain language summary
    "legal_type": str | None,           # ðŸ¤– TODO: "criminal_statute", "civil_statute", etc.
    "practice_areas": list[str] | None, # ðŸ¤– TODO: ["criminal_law", "property_law", ...]
    "complexity": int | None,           # ðŸ¤– TODO: 1-10 scale
    "key_terms": list[str] | None,      # ðŸ¤– TODO: Extracted important terms

    # Criminal law specific
    "offense_level": str | None,        # ðŸ¤– TODO: "felony", "misdemeanor" (if applicable)
    "offense_degree": str | None        # ðŸ¤– TODO: "F1"-"F5", "M1"-"M4" (if applicable)
  }
}
```

**Implementation Approach:**

1. **Batch Enrichment Pipeline:**
   ```python
   # Use auto_enricher.py (already exists in ohio_caselaw/lmdb/)
   from auto_enricher import LMDBEnricher

   enricher = LMDBEnricher(
       lmdb_path="path/to/corpus.lmdb",
       model="claude-3-5-sonnet-20241022"
   )

   enricher.enrich_all_sections(batch_size=100)
   ```

2. **Prompts for Enrichment:**
   ```
   Analyze this legal text and provide:
   1. A 1-2 sentence plain language summary
   2. Legal type (criminal_statute, civil_statute, definitional, procedural)
   3. Practice areas (list of relevant legal practice areas)
   4. Complexity (1-10 scale, where 1=simple, 10=extremely complex)
   5. Key legal terms (important terms a lawyer would search for)
   6. If criminal statute: offense level and degree
   ```

3. **Storage Strategy:**
   ```python
   # Store as nested JSON in enrichment field
   section_data["enrichment"] = {
       "summary": "Defines the crime of theft...",
       "legal_type": "criminal_statute",
       "practice_areas": ["criminal_law"],
       "complexity": 4,
       "key_terms": ["theft", "property", "deprive"],
       "offense_level": "felony",
       "offense_degree": "F5"
   }
   ```

**Effort:** MEDIUM-HIGH (LLM API costs, batch processing time)
**Benefit:** Semantic search, plain language summaries, practice area filtering

---

#### ðŸ”— Phase 5: Relationship Tracking (ADVANCED)

```python
{
  # Legal relationship tracking
  "invalidated_by": str | None,   # ðŸ”® TODO: Reference to invalidating case/law
  "superseded_by": str | None,    # ðŸ”® TODO: Reference to superseding law
}
```

**Implementation Notes:**

- **Requires:** Treatment status analysis (Phase 2) + citation context analysis
- **Detection:** Look for phrases like "overruled", "superseded by", "no longer valid"
- **Validation:** May require manual review for critical relationships

**Effort:** HIGH (relationship detection, validation)
**Benefit:** Temporal validity, avoid citing invalid law

---

## CITATIONS.LMDB Schema

### âœ… Currently Populated

```python
{
  "section": str,                     # âœ… Source section ID
  "direct_references": list[str],     # âœ… Referenced section IDs
  "reference_count": int,             # âœ… Count
  "references_details": [             # âœ… Enhanced details
    {
      "section": str,                 # âœ… Target section
      "title": str,                   # âœ… Target title
      "url": str,                     # âœ… Target URL
      "url_hash": str,                # âœ… URL hash
      "relationship": str,            # âœ… Type: "cross_reference", "defines", "cites"
      "context": str,                 # âœ… Surrounding text (100 chars)
      "position": int                 # âœ… Position in source (0-indexed)
    }
  ]
}
```

**Status:** COMPLETE âœ… (all fields populated by builders)

---

## REVERSE_CITATIONS.LMDB Schema

### âœ… Currently Populated

```python
{
  "section": str,                     # âœ… Target section ID
  "cited_by": list[str],              # âœ… Citing section IDs (sorted)
  "cited_by_count": int,              # âœ… Count
  "citing_details": [                 # âœ… Details
    {
      "section": str,                 # âœ… Citing section
      "title": str,                   # âœ… Citing section title
      "url": str                      # âœ… Citing section URL
    }
  ]
}
```

**Status:** COMPLETE âœ… (supports backward traversal)

---

## CHAINS.LMDB Schema

### âœ… Currently Populated

```python
{
  "chain_id": str,                    # âœ… Primary section ID
  "primary_section": str,             # âœ… Starting point
  "chain_sections": list[str],        # âœ… All sections in chain (ordered)
  "chain_depth": int,                 # âœ… Number of sections
  "references_count": int,            # âœ… Total references
  "created_at": str,                  # âœ… ISO8601 timestamp
  "complete_chain": [                 # âœ… Full text of all sections
    {
      "section": str,                 # âœ… Section ID
      "title": str,                   # âœ… Title
      "url": str,                     # âœ… URL
      "url_hash": str,                # âœ… Hash
      "full_text": str,               # âœ… Complete legal text
      "word_count": int               # âœ… Word count
    }
  ]
}
```

**Status:** COMPLETE âœ… (supports complex chain traversal)

---

## METADATA.LMDB Schema

### âœ… Currently Populated

```python
{
  # Key: b'corpus_info'
  "total_sections": int,              # âœ… Total items
  "sections_with_citations": int,     # âœ… Items citing others
  "complex_chains": int,              # âœ… Chain count
  "reverse_citations": int,           # âœ… Items cited by others
  "build_date": str,                  # âœ… ISO8601 timestamp
  "source": str,                      # âœ… Source URL
  "version": str,                     # âœ… Builder version
  "builder": str,                     # âœ… Builder script name
  "databases": dict                   # âœ… Database descriptions
}

{
  # Key: b'inbound_count_{section_id}'
  "section": str,                     # âœ… Section ID
  "count": int                        # âœ… Times cited
}
```

**Status:** COMPLETE âœ…

---

## Implementation Priority

### Immediate (This Week):
1. âœ… Core storage fields (DONE)
2. ðŸ“‹ Extract `chapter` and `title` from existing data (Phase 2)
3. ðŸ“‹ Add basic `treatment_status` = "valid" (Phase 2)

### Short Term (Next 2 Weeks):
4. ðŸ“‹ Court hierarchy for case law (Phase 2)
5. ðŸ¤– Start enrichment pipeline (Phase 4) - run on small batch first

### Medium Term (Next Month):
6. ðŸ§® Graph metrics computation (Phase 3)
7. ðŸ¤– Full corpus enrichment (Phase 4)

### Long Term (Future):
8. ðŸ”— Relationship tracking (Phase 5)
9. ðŸ”— Temporal validity windows (Phase 5)

---

## UI Graph Visualization Requirements

Based on your vision, the LMDB data supports:

### âœ… Already Supported:
- **Bidirectional traversal:** citations.lmdb (forward) + reverse_citations.lmdb (backward)
- **Clickability detection:** `is_clickable` field in primary.lmdb
- **Citation chains:** chains.lmdb for complex relationships
- **Relationship context:** `relationship` and `context` fields in citation details

### ðŸ“‹ Needs Phase 2:
- **Corpus color-coding:** Use `chapter` or corpus identifier for color mapping
- **Court level indication:** `court_level` for different node styling

### ðŸ§® Needs Phase 3:
- **Node importance sizing:** Use `authority_score` for node size
- **Depth-based opacity:** Compute from graph traversal distance + use `betweenness_centrality`

### Graph Traversal Algorithm (Pseudocode):

```python
def build_interactive_graph(clicked_section_id, depth=3):
    """
    Build graph data for UI visualization

    Returns:
    {
      "nodes": [
        {
          "id": "2913.02",
          "title": "Theft",
          "corpus": "ohio_revised",
          "color": "#FF5733",  # Based on corpus
          "opacity": 1.0,       # Distance 0 from clicked node
          "size": 50,           # Based on authority_score
          "court_level": null   # For case law only
        },
        {
          "id": "2913.01",
          "title": "Definitions",
          "corpus": "ohio_revised",
          "color": "#FF5733",
          "opacity": 0.7,       # Distance 1 from clicked node
          "size": 30,
          "court_level": null
        }
      ],
      "edges": [
        {
          "source": "2913.02",
          "target": "2913.01",
          "relationship": "defines",
          "context": "As used in sections 2913.01...",
          "weight": 1.0
        }
      ]
    }
    """

    # 1. Get clicked section data
    section = primary_db.get(clicked_section_id)

    # 2. Get forward citations (what this cites)
    forward_citations = citations_db.get(clicked_section_id)

    # 3. Get reverse citations (what cites this)
    reverse_citations = reverse_citations_db.get(clicked_section_id)

    # 4. Build graph with BFS up to depth limit
    graph = {
        "nodes": [],
        "edges": []
    }

    visited = set()
    queue = [(clicked_section_id, 0)]  # (section_id, distance)

    while queue:
        current_id, distance = queue.pop(0)

        if current_id in visited or distance > depth:
            continue

        visited.add(current_id)

        # Get section data
        section_data = primary_db.get(current_id)

        # Calculate opacity based on distance
        opacity = 1.0 - (distance * 0.2)  # Decrease by 0.2 per level

        # Add node
        graph["nodes"].append({
            "id": current_id,
            "title": section_data["section_title"],
            "corpus": extract_corpus(current_id),
            "color": get_corpus_color(extract_corpus(current_id)),
            "opacity": max(0.2, opacity),
            "size": section_data.get("authority_score", 0.5) * 100,
            "court_level": section_data.get("court_level")
        })

        # Add forward edges
        citations = citations_db.get(current_id)
        if citations:
            for ref in citations["references_details"]:
                graph["edges"].append({
                    "source": current_id,
                    "target": ref["section"],
                    "relationship": ref["relationship"],
                    "context": ref["context"],
                    "weight": 1.0
                })

                # Add to queue for BFS
                if distance < depth:
                    queue.append((ref["section"], distance + 1))

        # Add backward edges
        reverse = reverse_citations_db.get(current_id)
        if reverse:
            for citing in reverse["citing_details"]:
                graph["edges"].append({
                    "source": citing["section"],
                    "target": current_id,
                    "relationship": "cites",
                    "context": "",
                    "weight": 1.0
                })

                if distance < depth:
                    queue.append((citing["section"], distance + 1))

    return graph

# Corpus color mapping
CORPUS_COLORS = {
    "ohio_revised": "#FF5733",      # Red-orange
    "ohio_administration": "#33C3FF", # Light blue
    "ohio_constitution": "#FFD700",  # Gold
    "ohio_caselaw": "#9B59B6"        # Purple
}
```

---

## LLM Research Workflow

Your LMDB structure supports deep analysis workflows:

### Workflow Example: "What are the penalties for theft in Ohio?"

```python
async def perform_legal_research(user_query: str):
    """
    Multi-step research workflow using LMDB data + LLM
    """

    # Step 1: Vector search for relevant sections
    relevant_sections = await vector_search(user_query, top_k=5)
    # Returns: ["2913.02", "2913.03", "2913.51", ...]

    # Step 2: Fetch full section data from LMDB
    sections_data = []
    for section_id in relevant_sections:
        section = primary_db.get(section_id)
        sections_data.append({
            "id": section_id,
            "title": section["section_title"],
            "text": section["full_text"],
            "enrichment": section.get("enrichment", {}),
            "authority_score": section.get("authority_score", 0),
            "citation_count": section["citation_count"]
        })

    # Step 3: Get citation context (related sections)
    citation_context = []
    for section_id in relevant_sections:
        # Get what this section references
        citations = citations_db.get(section_id)
        if citations:
            for ref in citations["references_details"][:3]:  # Top 3
                ref_section = primary_db.get(ref["section"])
                citation_context.append({
                    "id": ref["section"],
                    "title": ref_section["section_title"],
                    "relationship": ref["relationship"],
                    "context": ref["context"]
                })

        # Get case law citing this statute
        reverse = reverse_citations_db.get(section_id)
        if reverse:
            for citing in reverse["citing_details"][:3]:
                citing_section = primary_db.get(citing["section"])
                citation_context.append({
                    "id": citing["section"],
                    "title": citing_section["section_title"],
                    "type": "case_law_application"
                })

    # Step 4: Build comprehensive context for LLM
    llm_context = f"""
    User Query: {user_query}

    Relevant Statutes:
    {format_sections(sections_data)}

    Related Provisions:
    {format_citations(citation_context)}

    Enrichment Data:
    {format_enrichment(sections_data)}
    """

    # Step 5: LLM Analysis
    analysis = await llm.analyze(
        prompt="""You are an expert Ohio legal researcher. Based on the provided
        statutes, case law, and citation relationships, provide a comprehensive
        answer to the user's question. Include:

        1. Direct answer with specific statute references
        2. Explanation of relevant provisions
        3. How courts have applied these statutes (case law)
        4. Practical implications
        5. Related areas of law the user should be aware of
        """,
        context=llm_context
    )

    # Step 6: Return rich response with citations
    return {
        "answer": analysis,
        "primary_sources": sections_data,
        "related_sections": citation_context,
        "graph_data": build_interactive_graph(relevant_sections[0]),
        "confidence_score": calculate_confidence(sections_data)
    }
```

### Key Features Enabled by Current Schema:

âœ… **Full text retrieval:** `full_text` field
âœ… **Citation network:** citations.lmdb + reverse_citations.lmdb
âœ… **Relationship context:** `relationship` and `context` fields
âœ… **Authority ranking:** `citation_count` (Phase 3: `authority_score`)
âœ… **Semantic understanding:** `enrichment` field (Phase 4)
âœ… **Practice area filtering:** `enrichment.practice_areas` (Phase 4)
âœ… **Chain analysis:** chains.lmdb for deep dependencies

---

## Summary

**Your schemas are CORRECT** - they represent the full vision.

**Current Status:**
- âœ… Core graph traversal: READY
- âœ… Citation relationships: COMPLETE
- âœ… Full text storage: COMPLETE
- ðŸ“‹ Metadata extraction: NEXT PRIORITY
- ðŸ¤– AI enrichment: MEDIUM TERM
- ðŸ§® Graph metrics: MEDIUM TERM

**Next Steps:**
1. Extract `chapter` and `title` from existing data (easy win)
2. Add basic `treatment_status` (default to "valid")
3. Start enrichment pipeline on small batch
4. Build graph visualization prototype using current data
5. Compute graph metrics once all corpora are loaded

The foundation is solid. You have everything needed for the graph visualization and LLM workflows - just need to populate the remaining fields incrementally.